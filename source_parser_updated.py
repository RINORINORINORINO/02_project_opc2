import requests
from bs4 import BeautifulSoup
import fitz  # PyMuPDF
import os
import docx
from typing import Union, Dict, List, Any, Optional, Tuple, Set
import time
import random
from urllib.parse import urlparse, urljoin
import re
from functools import lru_cache
import concurrent.futures
import logging
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from cloud_ocr import parse_cloud_ocr

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ìœ íŠœë¸Œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ - ë™ì  ì„í¬íŠ¸ ì²˜ë¦¬
youtube_parser = None

# íƒ€ì… ì •ì˜
SourceType = Union[str, Dict[str, str]]  # URL ë¬¸ìì—´ ë˜ëŠ” {'type': 'pdf', 'path': '...'} í˜•íƒœ

# ì´ ìœ„ì¹˜ì— ì¶”ê°€
SUPPORTED_FILE_TYPES = ['.pdf', '.docx', '.txt', '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']

# ì„¸ì…˜ ê°ì²´ ìƒì„± ë° ì¬ì‹œë„ ì„¤ì •
def create_session() -> requests.Session:
    """í–¥ìƒëœ ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” ì„¸ì…˜ ìƒì„±"""
    session = requests.Session()
    retry_strategy = Retry(
        total=3,  # ì´ ì‹œë„ íšŸìˆ˜
        backoff_factor=0.5,  # ì¬ì‹œë„ ê°„ê²© ì¦ê°€ ì¸ì
        status_forcelist=[429, 500, 502, 503, 504],  # ì¬ì‹œë„í•  HTTP ìƒíƒœ ì½”ë“œ
        allowed_methods=["GET", "POST"]  # ì¬ì‹œë„í•  HTTP ë©”ì„œë“œ
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

# ê¸°ë³¸ ì„¸ì…˜ ìƒì„±
session = create_session()

@lru_cache(maxsize=32)
def parse_url(url: str) -> str:
    """
    ì›¹ URLë¡œë¶€í„° ê¸°ì‚¬/ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ - ìºì‹± ë° ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”
    
    Args:
        url: íŒŒì‹±í•  ì›¹ URL
        
    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì»¨í…ì¸ 
    """
    # YouTube URL í™•ì¸
    if "youtube.com" in url or "youtu.be" in url:
        logger.info(f"ğŸ¬ YouTube ì˜ìƒ URL ê°ì§€: {url}")
        return parse_youtube_content(url)
    
    try:
        logger.info(f"ğŸŒ URL íŒŒì‹± ì‹œì‘: {url[:60]}{'...' if len(url) > 60 else ''}")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Referer": "https://www.google.com/",
            "DNT": "1",
        }
        
        res = session.get(url, headers=headers, timeout=15)
        res.raise_for_status()  # ì˜¤ë¥˜ ìƒíƒœ ì½”ë“œ í™•ì¸
        
        # ì¸ì½”ë”© ì²˜ë¦¬ (ëª…ì‹œì  ì¸ì½”ë”©ì´ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„)
        if res.encoding.lower() == 'iso-8859-1':
            # ì¸ì½”ë”© ê°ì§€ ì‹œë„
            res.encoding = res.apparent_encoding
        
        soup = BeautifulSoup(res.text, "html.parser")
        
        # ë¯¸ë””ì–´ í”Œë«í¼ë³„ ìµœì í™”ëœ íŒŒì‹±
        domain = urlparse(url).netloc
        
        # íŠ¹ì • ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ìµœì í™”
        if "medium.com" in domain:
            content = parse_medium(soup)
        elif any(x in domain for x in ["nytimes.com", "washingtonpost.com", "theguardian.com"]):
            content = parse_news_site(soup)
        elif "wikipedia.org" in domain:
            content = parse_wikipedia(soup)
        elif "arxiv.org" in domain:
            content = parse_arxiv(soup)
        else:
            # ì¼ë°˜ì ì¸ íŒŒì‹± ë°©ë²•
            content = general_parsing(soup, url)
            
        if not content:
            logger.warning(f"âš ï¸ {url}ì—ì„œ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ íŒŒì‹±ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
            content = general_parsing(soup, url)
            
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        content = clean_text(content)
        
        logger.info(f"âœ… URL íŒŒì‹± ì™„ë£Œ: {url[:60]}{'...' if len(url) > 60 else ''}")
        return content

    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ URL ìš”ì²­ ì˜¤ë¥˜ ({url}): {str(e)}")
        return f"URL ì ‘ê·¼ ì˜¤ë¥˜: {url}\nì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­: {str(e)}"
    except Exception as e:
        logger.error(f"âŒ URL íŒŒì‹± ì˜¤ë¥˜ ({url}): {str(e)}")
        return f"URL íŒŒì‹± ì˜¤ë¥˜: {url}\nì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­: {str(e)}"

def parse_medium(soup: BeautifulSoup) -> str:
    """Medium ì•„í‹°í´ íŒŒì‹±"""
    article = soup.select_one("article")
    if article:
        # Medium ì œëª© ì¶”ì¶œ
        title_elem = soup.select_one("h1")
        title = title_elem.get_text(strip=True) if title_elem else ""
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        paragraphs = article.select("p")
        content = "\n".join(p.get_text(strip=True) for p in paragraphs)
        
        if title:
            return f"{title}\n\n{content}"
        return content
    return ""

def parse_news_site(soup: BeautifulSoup) -> str:
    """ì£¼ìš” ë‰´ìŠ¤ ì‚¬ì´íŠ¸ íŒŒì‹±"""
    # ê¸°ì‚¬ ì œëª©
    title_elem = soup.select_one("h1") or soup.select_one(".headline") or soup.select_one(".title")
    title = title_elem.get_text(strip=True) if title_elem else ""
    
    # ê¸°ì‚¬ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
    article = soup.select_one("article") or soup.select_one(".article-body") or soup.select_one(".story-body")
    
    if article:
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for el in article.select(".ad, .advertisement, .social-share, .newsletter"):
            if el:
                el.decompose()
        
        paragraphs = article.select("p")
        content = "\n".join(p.get_text(strip=True) for p in paragraphs)
        
        if title:
            return f"{title}\n\n{content}"
        return content
    return ""

def parse_wikipedia(soup: BeautifulSoup) -> str:
    """ìœ„í‚¤í”¼ë””ì•„ íŒŒì‹±"""
    # ì œëª© ì¶”ì¶œ
    title_elem = soup.select_one("#firstHeading")
    title = title_elem.get_text(strip=True) if title_elem else ""
    
    content_div = soup.select_one("#mw-content-text")
    if content_div:
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for el in content_div.select(".reference, .mw-editsection, .thumb, .navbox, .vertical-navbox"):
            el.decompose()
        
        # ë³¸ë¬¸ ë¬¸ë‹¨ ì¶”ì¶œ
        paragraphs = content_div.select("p")
        content = "\n".join(p.get_text(strip=True) for p in paragraphs)
        
        # ëª©ì°¨ ì¶”ì¶œ
        toc = []
        for heading in content_div.select("h2, h3"):
            if heading.get_text(strip=True) not in ["Contents", "References", "External links"]:
                toc.append(heading.get_text(strip=True).replace("[edit]", ""))
        
        toc_text = "ëª©ì°¨: " + ", ".join(toc) if toc else ""
        
        result = []
        if title:
            result.append(title)
        if toc_text:
            result.append(toc_text)
        if content:
            result.append(content)
            
        return "\n\n".join(result)
    return ""

def parse_arxiv(soup: BeautifulSoup) -> str:
    """arXiv ë…¼ë¬¸ íŒŒì‹±"""
    # ë…¼ë¬¸ ì œëª©
    title_elem = soup.select_one(".title")
    title = title_elem.get_text(strip=True).replace("Title:", "").strip() if title_elem else ""
    
    # ì €ì
    authors_elem = soup.select_one(".authors")
    authors = authors_elem.get_text(strip=True).replace("Authors:", "").strip() if authors_elem else ""
    
    # ì´ˆë¡
    abstract = soup.select_one(".abstract")
    abstract_text = ""
    if abstract:
        abstract_text = abstract.get_text(strip=True).replace("Abstract: ", "")
    
    # ì¡°í•©
    result = []
    if title:
        result.append(f"ì œëª©: {title}")
    if authors:
        result.append(f"ì €ì: {authors}")
    if abstract_text:
        result.append(f"ì´ˆë¡:\n{abstract_text}")
        
    return "\n\n".join(result)

def general_parsing(soup: BeautifulSoup, url: str) -> str:
    """ì¼ë°˜ì ì¸ ì›¹í˜ì´ì§€ íŒŒì‹± ë°©ë²• - ë„ë©”ì¸ë³„ íœ´ë¦¬ìŠ¤í‹± ê°œì„ """
    # ì œëª© ì¶”ì¶œ
    title = ""
    title_tag = soup.find("title")
    if title_tag:
        title = title_tag.get_text(strip=True)
    
    # meta description ì¶”ì¶œ (í˜ì´ì§€ ìš”ì•½ì— ìœ ìš©)
    meta_desc = ""
    meta_tag = soup.find("meta", attrs={"name": "description"}) or soup.find("meta", attrs={"property": "og:description"})
    if meta_tag and meta_tag.get("content"):
        meta_desc = meta_tag["content"]
    
    # ë„ë©”ì¸ ì •ë³´ ì¶”ì¶œ (ì‚¬ì´íŠ¸ íŠ¹ì„± íŒŒì•…)
    domain = urlparse(url).netloc
    
    # ë„ë©”ì¸ë³„ ì»¤ìŠ¤í…€ ì»¨í…ì¸  ì„ íƒì
    domain_selectors = {
        "bbc.com": "article .ssrcss-uf6wea-RichTextComponentWrapper",
        "cnn.com": ".article__content",
        "reuters.com": ".article-body__content__17Yit",
        "bloomberg.com": ".body-content",
        "apnews.com": ".Article",
        "ft.com": ".article__content-body",
    }
    
    # ë„ë©”ì¸ë³„ ë§ì¶¤ ì„ íƒì ì‚¬ìš©
    for site, selector in domain_selectors.items():
        if site in domain:
            content_elems = soup.select(selector)
            if content_elems:
                return title + "\n\n" + "\n".join(elem.get_text(strip=True) for elem in content_elems)
    
    # ì—¬ëŸ¬ ë‚´ìš© ì»¨í…Œì´ë„ˆ í›„ë³´ ì‹œë„ (ìš°ì„ ìˆœìœ„ ìˆœ)
    content_candidates = [
        soup.select("article p"),
        soup.select("main p"),
        soup.select(".content p, .post-content p, .entry-content p, .article p"),
        soup.select(".story p, .body p, .post p"),
        soup.select('[role="main"] p'),
        soup.select('[role="article"] p'),
        soup.select("p")  # ë§ˆì§€ë§‰ ìˆ˜ë‹¨
    ]
    
    for candidate in content_candidates:
        if candidate and len("".join(p.get_text() for p in candidate)) > 200:
            content = "\n".join(p.get_text(strip=True) for p in candidate)
            if title:
                return title + "\n\n" + content
            return content
    
    # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ ì¶”ì¶œ
    try:
        # ë³¸ë¬¸ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ì˜ì—­ ì‹ë³„
        main_content = identify_main_content(soup)
        
        if main_content:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
            text = main_content.get_text(separator="\n", strip=True)
            lines = [line.strip() for line in text.split("\n") if len(line.strip()) > 30]
            
            if lines:
                if title:
                    return title + "\n\n" + "\n".join(lines)
                return "\n".join(lines)
    except Exception:
        pass
    
    # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ë©”íƒ€ ì •ë³´ë¼ë„ ë°˜í™˜
    if title or meta_desc:
        result = []
        if title:
            result.append(f"ì œëª©: {title}")
        if meta_desc:
            result.append(f"ì„¤ëª…: {meta_desc}")
        result.append(f"URL: {url}")
        result.append("(ì›¹í˜ì´ì§€ì—ì„œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)")
        return "\n\n".join(result)
    
    return f"ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {url}"

def identify_main_content(soup: BeautifulSoup) -> Optional[BeautifulSoup]:
    """
    ë³¸ë¬¸ ì½˜í…ì¸ ë¡œ ì¶”ì •ë˜ëŠ” ì˜ì—­ì„ ì‹ë³„í•˜ëŠ” í•¨ìˆ˜
    í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ë°€ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨
    """
    # ì£¼ìš” ì½˜í…ì¸  í›„ë³´ íƒœê·¸
    candidates = {}
    
    # ì¼ë°˜ì ì¸ ì½˜í…ì¸  ì»¨í…Œì´ë„ˆ íƒœê·¸
    for tag in ["article", "main", "div", "section"]:
        for element in soup.find_all(tag):
            # í´ë˜ìŠ¤/ID ë¶„ì„í•˜ì—¬ ê´‘ê³ , ë„¤ë¹„ê²Œì´ì…˜ ë“± ì œì™¸
            if element.get("class"):
                class_str = " ".join(element.get("class")).lower()
                if any(x in class_str for x in ["nav", "menu", "sidebar", "footer", "comment", "ad-", "advertisement"]):
                    continue
            
            # id ë¶„ì„
            if element.get("id"):
                id_str = element.get("id").lower()
                if any(x in id_str for x in ["nav", "menu", "sidebar", "footer", "comment"]):
                    continue
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
            text = element.get_text(" ", strip=True)
            if len(text) < 100:  # ë„ˆë¬´ ì§§ì€ ì½˜í…ì¸  ì œì™¸
                continue
                
            # í…ìŠ¤íŠ¸ ë°€ë„ ê³„ì‚° (í…ìŠ¤íŠ¸ ê¸¸ì´ / HTML ê¸¸ì´)
            html_len = len(str(element))
            if html_len == 0:
                continue
                
            text_density = len(text) / html_len
            
            # ì ìˆ˜ ê³„ì‚° (í…ìŠ¤íŠ¸ ê¸¸ì´ * í…ìŠ¤íŠ¸ ë°€ë„)
            score = len(text) * text_density
            candidates[element] = score
    
    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    if candidates:
        return max(candidates.items(), key=lambda x: x[1])[0]
    
    return None

def parse_pdf(path: str) -> str:
    """PDF íŒŒì¼ ê²½ë¡œì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ - í–¥ìƒëœ ë²„ì „"""
    try:
        logger.info(f"ğŸ“„ PDF íŒŒì‹± ì‹œì‘: {os.path.basename(path)}")
        doc = fitz.open(path)
        texts = []
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = doc.metadata
        if metadata.get("title"):
            texts.append(f"ì œëª©: {metadata.get('title')}")
        if metadata.get("author"):
            texts.append(f"ì €ì: {metadata.get('author')}")
        if metadata.get("subject"):
            texts.append(f"ì£¼ì œ: {metadata.get('subject')}")
        texts.append("")  # ë¹ˆ ì¤„ ì¶”ê°€
        
        # TOC(ëª©ì°¨) ì¶”ì¶œ ì‹œë„
        toc = doc.get_toc()
        if toc:
            texts.append("ëª©ì°¨:")
            for level, title, page in toc:
                indent = "  " * (level - 1)
                texts.append(f"{indent}- {title} (í˜ì´ì§€: {page})")
            texts.append("")  # ë¹ˆ ì¤„ ì¶”ê°€
        
        # ë³¸ë¬¸ ì¶”ì¶œ - í…ìŠ¤íŠ¸ ë¸”ë¡ ê¸°ë°˜ ì ‘ê·¼
        for page_num, page in enumerate(doc):
            # í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ì„ í†µí•œ í–¥ìƒëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            blocks = page.get_text("blocks")
            page_text = []
            
            for block in blocks:
                if block[6] == 0:  # í…ìŠ¤íŠ¸ ë¸”ë¡
                    block_text = block[4].strip()
                    if block_text:
                        page_text.append(block_text)
            
            if page_text:
                # í˜ì´ì§€ ë²ˆí˜¸ í‘œì‹œ (íŠ¹íˆ ê¸´ ë¬¸ì„œì—ì„œ ìœ ìš©)
                if len(doc) > 5:  # í˜ì´ì§€ê°€ 5ê°œ ì´ìƒì¸ ê²½ìš°ì—ë§Œ
                    texts.append(f"--- í˜ì´ì§€ {page_num + 1} ---")
                texts.append("\n".join(page_text))
        
        logger.info(f"âœ… PDF íŒŒì‹± ì™„ë£Œ: {os.path.basename(path)}")
        return "\n".join(texts)
    except Exception as e:
        logger.error(f"âŒ PDF íŒŒì‹± ì˜¤ë¥˜ ({path}): {str(e)}")
        return f"PDF íŒŒì‹± ì˜¤ë¥˜: {path}\nì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­: {str(e)}"

def parse_docx(path: str) -> str:
    """DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ - ìŠ¤íƒ€ì¼ê³¼ êµ¬ì¡° ì •ë³´ í¬í•¨"""
    try:
        logger.info(f"ğŸ“„ DOCX íŒŒì‹± ì‹œì‘: {os.path.basename(path)}")
        doc = docx.Document(path)
        full_text = []
        
        # ë¬¸ì„œ ì†ì„± ì¶”ì¶œ
        properties = doc.core_properties
        if properties.title:
            full_text.append(f"ì œëª©: {properties.title}")
        if properties.author:
            full_text.append(f"ì €ì: {properties.author}")
        if properties.comments:
            full_text.append(f"ì„¤ëª…: {properties.comments}")
        full_text.append("")  # ë¹ˆ ì¤„ ì¶”ê°€
        
        # ì œëª©ê³¼ í—¤ë” ì‹ë³„í•˜ì—¬ êµ¬ì¡°ì  í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for para in doc.paragraphs:
            if not para.text.strip():
                continue
                
            # ìŠ¤íƒ€ì¼ ê¸°ë°˜ êµ¬ì¡° ì‹ë³„
            style = para.style.name
            text = para.text.strip()
            
            # ì œëª© ìŠ¤íƒ€ì¼ì´ë©´ ë§ˆí¬ë‹¤ìš´ í—¤ë”©ìœ¼ë¡œ ë³€í™˜
            if "Heading" in style:
                level = int(style.replace("Heading", "").strip()) if style.replace("Heading", "").strip().isdigit() else 1
                heading_marker = "#" * min(level, 6)
                full_text.append(f"{heading_marker} {text}")
            else:
                full_text.append(text)
        
        # í…Œì´ë¸” ì²˜ë¦¬
        for i, table in enumerate(doc.tables):
            full_text.append(f"\ní‘œ {i+1}:")
            for row in table.rows:
                row_text = []
                for cell in row.cells:
                    if cell.text.strip():
                        row_text.append(cell.text.strip())
                if row_text:
                    full_text.append(" | ".join(row_text))
        
        logger.info(f"âœ… DOCX íŒŒì‹± ì™„ë£Œ: {os.path.basename(path)}")
        return "\n".join(full_text)
    except Exception as e:
        logger.error(f"âŒ DOCX íŒŒì‹± ì˜¤ë¥˜ ({path}): {str(e)}")
        return f"DOCX íŒŒì‹± ì˜¤ë¥˜: {path}\nì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­: {str(e)}"

def parse_txt(path: str) -> str:
    """TXT íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ - ë‹¤ì–‘í•œ ì¸ì½”ë”© ì§€ì›"""
    logger.info(f"ğŸ“„ TXT íŒŒì‹± ì‹œì‘: {os.path.basename(path)}")
    
    # ì‹œë„í•  ì¸ì½”ë”© ëª©ë¡
    encodings = ['utf-8', 'latin-1', 'cp949', 'euc-kr', 'cp1252']
    
    for encoding in encodings:
        try:
            with open(path, 'r', encoding=encoding) as file:
                content = file.read()
            
            logger.info(f"âœ… TXT íŒŒì‹± ì™„ë£Œ ({encoding} ì¸ì½”ë”©): {os.path.basename(path)}")
            
            # íŒŒì¼ ê¸°ë³¸ ì •ë³´ ì¶”ê°€
            file_info = f"íŒŒì¼ëª…: {os.path.basename(path)}\n"
            file_info += f"í¬ê¸°: {os.path.getsize(path) / 1024:.1f} KB\n\n"
            
            return file_info + content
        except UnicodeDecodeError:
            continue
        except Exception as e:
            logger.error(f"âŒ TXT íŒŒì‹± ì˜¤ë¥˜ ({path}, {encoding}): {str(e)}")
    
    # ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨
    logger.error(f"âŒ TXT íŒŒì‹± ì‹¤íŒ¨: ì§€ì›ë˜ëŠ” ì¸ì½”ë”©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ({path})")
    return f"TXT íŒŒì‹± ì˜¤ë¥˜: {path}\nì§€ì›ë˜ëŠ” ì¸ì½”ë”©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì •ê·œí™” - ê³ ê¸‰ ì •ì œ ê¸°ëŠ¥ ì¶”ê°€"""
    if not text:
        return ""
        
    # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'\s{2,}', ' ', text)
    
    # ì¼ë°˜ì ì¸ ì“¸ëª¨ì—†ëŠ” í…ìŠ¤íŠ¸ ì œê±° (ì˜ˆ: ì¿ í‚¤ ì •ì±…, êµ¬ë… ì•ˆë‚´ ë“±)
    patterns_to_remove = [
        r'ì¿ í‚¤ë¥¼ ì‚¬ìš©.*?ë™ì˜',
        r'Subscribe to.*?newsletter',
        r'êµ¬ë….*?ë‰´ìŠ¤ë ˆí„°',
        r'Published:.*?\d{4}',
        r'Last modified on.*?\d{4}',
        r'Share on (?:Twitter|Facebook|LinkedIn)',
        r'\d+ shares',
        r'Â©.*?All rights reserved',
        r'Terms of (?:use|service)',
        r'Privacy Policy',
        r'All Rights Reserved',
        r'Please enable JavaScript',
        r'You need to enable JavaScript',
        r'ADVERTISEMENT',
        r'Advertisement',
        r'Sponsored Content',
        r'Click here to view'
    ]
    
    for pattern in patterns_to_remove:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # ì¤‘ë³µëœ í…ìŠ¤íŠ¸ ë¸”ë¡ ì œê±° (íŠ¹íˆ PDFì—ì„œ ìì£¼ ë°œìƒ)
    lines = text.split('\n')
    unique_lines = []
    seen_chunks = set()
    
    for line in lines:
        line = line.strip()
        if len(line) > 20:  # ê¸´ ì¤„ì— ëŒ€í•´ì„œë§Œ ì¤‘ë³µ ê²€ì‚¬
            # ì¤„ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²´í¬ (ì•„ì£¼ ê¸´ ì¤„ì¸ ê²½ìš° ì¼ë¶€ë§Œ ì¤‘ë³µë  ìˆ˜ë„ ìˆìŒ)
            chunk_size = 50
            chunks = [line[i:i+chunk_size] for i in range(0, len(line), chunk_size)]
            
            # ì²« ë²ˆì§¸ ì²­í¬ê°€ ì¤‘ë³µë˜ë©´ ê±´ë„ˆëœ€
            if chunks and chunks[0] in seen_chunks:
                continue
            
            # ì²­í¬ ì¶”ê°€
            for chunk in chunks:
                if len(chunk) >= 20:  # ì˜ë¯¸ ìˆëŠ” í¬ê¸°ì˜ ì²­í¬ë§Œ ì²´í¬
                    seen_chunks.add(chunk)
        
        unique_lines.append(line)
    
    # ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ë°˜í™˜
    cleaned_text = '\n'.join(unique_lines)
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)  # ìµœì¢… ì¤„ë°”ê¿ˆ ì •ë¦¬
    
    return cleaned_text.strip()

def parse_youtube_content(url: str) -> str:
    """YouTube ì˜ìƒ ì½˜í…ì¸  ì¶”ì¶œ - ë™ì  ì„í¬íŠ¸ ì²˜ë¦¬"""
    global youtube_parser
    
    # í•„ìš”ì‹œ ìœ íŠœë¸Œ íŒŒì„œ ëª¨ë“ˆ ì„í¬íŠ¸
    if youtube_parser is None:
        try:
            import youtube_parser as youtube_parser_module
            youtube_parser = youtube_parser_module
            logger.info("âœ… YouTube íŒŒì„œ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
        except ImportError:
            logger.error("âŒ YouTube íŒŒì„œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return f"YouTube ì˜ìƒ íŒŒì‹± ì˜¤ë¥˜ ({url}): YouTube íŒŒì„œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        return youtube_parser.parse_youtube(url)
    except Exception as e:
        logger.error(f"âŒ YouTube ì˜ìƒ íŒŒì‹± ì˜¤ë¥˜ ({url}): {str(e)}")
        return f"YouTube ì˜ìƒ íŒŒì‹± ì˜¤ë¥˜ ({url}): {str(e)}"

# source_parser_updated.py íŒŒì¼ì— ì¶”ê°€ ë˜ëŠ” ìˆ˜ì •í•  ë¶€ë¶„

def parse_sources(sources: List[SourceType], max_workers: int = 4) -> List[str]:
    """
    URL ë˜ëŠ” íŒŒì¼ ëª©ë¡ ì „ì²´ ì²˜ë¦¬ - ë³‘ë ¬ ì²˜ë¦¬ ë° ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”
    
    Args:
        sources: URL ë˜ëŠ” íŒŒì¼ ê²½ë¡œ ëª©ë¡
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—…ì ìˆ˜
        
    Returns:
        íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ëª©ë¡
    """
    parsed_texts = []
    successful_sources = 0
    failed_sources = 0
    
    # í´ë” ì²˜ë¦¬ë¥¼ ìœ„í•œ ì†ŒìŠ¤ í™•ì¥ (ì¶”ê°€ëœ ë¶€ë¶„)
    expanded_sources = []
    for src in sources:
        # ì´ë¯¸ì§€ í´ë” ì²˜ë¦¬
        if isinstance(src, dict) and src.get("type") == "image_folder":
            # í´ë” ë‚´ ê° ì´ë¯¸ì§€ íŒŒì¼ì„ ê°œë³„ ì†ŒìŠ¤ë¡œ ì¶”ê°€
            for file_path in src.get("files", []):
                expanded_sources.append({
                    "type": file_path.split('.')[-1].lower(),
                    "path": file_path,
                    "ocr_engine": src.get("ocr_engine", "google")
                })
        else:
            # ì¼ë°˜ ì†ŒìŠ¤ëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
            expanded_sources.append(src)
    
    # í™•ì¥ëœ ì†ŒìŠ¤ ëª©ë¡ìœ¼ë¡œ êµì²´
    sources = expanded_sources
    total = len(sources)
    
    logger.info(f"ğŸ”„ {total}ê°œ ì†ŒìŠ¤ íŒŒì‹± ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬: {max_workers}ê°œ ì‘ì—…ì)")
    
    # ê¸°ì¡´ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€...
    
    # ì†ŒìŠ¤ íƒ€ì…ì— ë”°ë¥¸ íŒŒì‹± í•¨ìˆ˜ ë§¤í•‘
    def parse_source(src_with_index: Tuple[int, SourceType]) -> Tuple[int, str, bool]:
        idx, src = src_with_index
        
        try:
            logger.info(f"[{idx+1}/{total}] ì†ŒìŠ¤ íŒŒì‹± ì¤‘...")
            
            if isinstance(src, str):
                # URL í™•ì¸
                if src.startswith(('http://', 'https://')):
                    parsed = parse_url(src)
                else:
                    logger.warning(f"âš ï¸ ì¸ì‹í•  ìˆ˜ ì—†ëŠ” ì†ŒìŠ¤ í˜•ì‹: {src}")
                    parsed = f"ì¸ì‹í•  ìˆ˜ ì—†ëŠ” ì†ŒìŠ¤ í˜•ì‹: {src}"
                    return idx, parsed, False
            elif isinstance(src, dict):
                src_type = src.get("type", "").lower()
                path = src.get("path", "")
                
                if not os.path.exists(path):
                    logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {path}")
                    return idx, f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {path}", False
                
                if src_type == "pdf" or path.lower().endswith(".pdf"):
                    parsed = parse_pdf(path)
                elif src_type == "docx" or path.lower().endswith(".docx"):
                    parsed = parse_docx(path)
                elif src_type == "txt" or path.lower().endswith(".txt"):
                    parsed = parse_txt(path)
                elif any(path.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']):
                    # ê¸°ë³¸ ì—”ì§„ìœ¼ë¡œ Google Vision ì‚¬ìš© (ì˜µì…˜ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥)
                    engine = src.get("ocr_engine", "google")  # "google", "aws", "azure", "naver" ì¤‘ ì„ íƒ
                    
                    # OCR ì—”ì§„ ê²€ì¦
                    valid_engines = ["google", "aws", "azure", "naver"]
                    if engine not in valid_engines:
                        logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” OCR ì—”ì§„: {engine}, ê¸°ë³¸ê°’ 'google'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                        engine = "google"
                    
                    logger.info(f"ğŸ” ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘: {os.path.basename(path)} (OCR ì—”ì§„: {engine})")
                    parsed = parse_cloud_ocr(path, engine=engine)
                else:
                    logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {src}")
                    parsed = f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {src.get('path', '')}"
                    return idx, parsed, False
            else:
                logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì†ŒìŠ¤ ìœ í˜•: {type(src)}")
                parsed = ""
                return idx, parsed, False

            # ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
            success = parsed and len(parsed) > 100  # ìµœì†Œ ê¸¸ì´ ê¸°ì¤€
            return idx, parsed, success
            
        except Exception as e:
            logger.error(f"âŒ ì†ŒìŠ¤ íŒŒì‹± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            return idx, f"íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", False
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=min(max_workers, total)) as executor:
        # ì‘ì—… ì œì¶œ
        future_to_idx = {executor.submit(parse_source, (i, src)): i for i, src in enumerate(sources)}
        
        # ê²°ê³¼ ìˆ˜ì§‘
        for future in concurrent.futures.as_completed(future_to_idx):
            try:
                idx, parsed_text, success = future.result()
                results.append((idx, parsed_text, success))
                
                if success:
                    successful_sources += 1
                    logger.info(f"âœ… ì†ŒìŠ¤ #{idx+1} íŒŒì‹± ì„±ê³µ")
                else:
                    failed_sources += 1
                    logger.warning(f"âš ï¸ ì†ŒìŠ¤ #{idx+1} íŒŒì‹± ê²°ê³¼ ë¶ˆì¶©ë¶„")
                
            except Exception as e:
                idx = future_to_idx[future]
                logger.error(f"âŒ ì†ŒìŠ¤ #{idx+1} ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                results.append((idx, f"íŒŒì‹± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}", False))
                failed_sources += 1
    
    # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    results.sort(key=lambda x: x[0])
    parsed_texts = [result[1] for result in results]
    
    logger.info(f"ğŸ ì†ŒìŠ¤ íŒŒì‹± ì™„ë£Œ: ì„±ê³µ {successful_sources}ê°œ, ì‹¤íŒ¨ {failed_sources}ê°œ")
    
    return parsed_texts

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    test_url = "https://en.wikipedia.org/wiki/Artificial_intelligence"
    logger.info("ìœ„í‚¤í”¼ë””ì•„ URL í…ŒìŠ¤íŠ¸ ì¤‘...")
    wiki_content = parse_url(test_url)
    logger.info(f"íŒŒì‹±ëœ ë‚´ìš© ì¼ë¶€:\n{wiki_content[:500]}...")
    
    # YouTube í…ŒìŠ¤íŠ¸
    youtube_url = "https://www.youtube.com/watch?v=dQw4w9WgXcQ"
    logger.info("\nYouTube URL í…ŒìŠ¤íŠ¸ ì¤‘...")
    try:
        yt_content = parse_url(youtube_url)
        logger.info(f"íŒŒì‹±ëœ ë‚´ìš© ì¼ë¶€:\n{yt_content[:500]}...")
    except Exception as e:
        logger.error(f"YouTube í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}")
    
    # íŒŒì¼ íŒŒì‹± í…ŒìŠ¤íŠ¸
    logger.info("\në‹¤ì¤‘ ì†ŒìŠ¤ ë³‘ë ¬ íŒŒì‹± í…ŒìŠ¤íŠ¸:")
    test_sources = [
        "https://en.wikipedia.org/wiki/Natural_language_processing",
        {"type": "txt", "path": "test_data/sample.txt"} if os.path.exists("test_data/sample.txt") else test_url
    ]
    
    parsed_results = parse_sources(test_sources)
    for i, result in enumerate(parsed_results):
        logger.info(f"ì†ŒìŠ¤ #{i+1} ê²°ê³¼ ì¼ë¶€:\n{result[:200]}...")